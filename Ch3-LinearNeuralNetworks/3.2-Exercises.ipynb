{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Linear Regression Implementation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What would happen if we were to initialize the weights to zero. Would the algorithm still work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the weights to zero would imply that `linreg(X, w, b)` return the same value (= `b`) for all `X`. Hence the loss function would be independent of `w`, which would mean that once the training is done, only `b` would have been optimized. When we then test out model, it would give the same prediction for any input (= `b` after training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Assume that you are Georg Simon Ohm trying to come up with a model between voltage and current. Can you use auto differentiation to learn the parameters of your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. Assume that a linear relationship exists between voltage V and current I. `V = wI + b`. Given a set of observations of V and I, our task is the to find the values of `(w, b)` which optimize our model. This can be achieved using auto-diff as illustrated in the text (linear regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Can you use Planckʼs Law to determine the temperature of an object using spectral energy density?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What are the problems you might encounter if you wanted to compute the second derivatives? How would  you fix them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second derivative much more expensive to compute than the first derivative, reason as mentioned in 2.2 Q1. \n",
    "\n",
    "How to fix them? TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Why is the `reshape` function needed in the `squared_loss` function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `reshape` is used to make `y` and `y_hat` the same shape for proper calculations (ex. if `y_hat` is a vector, and `y` is a 1D tensor)\n",
    "- `reshape` is used on `y` because to input to the function is `y_hat` and we expect the output to be that same shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Experiment using different learning rates to find out how fast the loss function value drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([4, 3.4, -3])\n",
    "true_b = torch.tensor([5.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "num_examples = 1000\n",
    "\n",
    "features, labels = synthetic_data(true_w, true_b, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(labels)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor([k for k in range(i, min(i + batch_size, num_examples))])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):\n",
    "    return((y_hat - y.reshape(y_hat.shape))**2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramter initialization has been done inside the function so that we start off from scratch everytime train() is called\n",
    "def train(lr):\n",
    "    w = torch.normal(0, 0.01, size=(len(true_w), 1), requires_grad=True)\n",
    "    b = torch.zeros(1, requires_grad=True)\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in data_iter(batch_size, features, labels):\n",
    "            l = squared_loss(linreg(X, w, b), y)\n",
    "            l.sum().backward()\n",
    "            sgd([w, b], lr, batch_size)\n",
    "        with torch.no_grad():\n",
    "            train_loss = squared_loss(linreg(features, w, b), labels)\n",
    "            if((epoch + 1) % 2 == 0):\n",
    "                print(f'epoch {epoch + 1}, loss = {float(train_loss.mean()):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate = 0.0003\n",
      "epoch 2, loss = 30.4014\n",
      "epoch 4, loss = 26.9095\n",
      "epoch 6, loss = 23.8204\n",
      "epoch 8, loss = 21.0873\n",
      "epoch 10, loss = 18.6692\n",
      "\n",
      "learning rate = 0.003\n",
      "epoch 2, loss = 10.1153\n",
      "epoch 4, loss = 3.0057\n",
      "epoch 6, loss = 0.8988\n",
      "epoch 8, loss = 0.2704\n",
      "epoch 10, loss = 0.0818\n",
      "\n",
      "learning rate = 0.03\n",
      "epoch 2, loss = 0.0002\n",
      "epoch 4, loss = 0.0000\n",
      "epoch 6, loss = 0.0000\n",
      "epoch 8, loss = 0.0000\n",
      "epoch 10, loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"learning rate = 0.0003\")\n",
    "train(0.0003)\n",
    "print(\"\\nlearning rate = 0.003\")\n",
    "train(0.003)\n",
    "print(\"\\nlearning rate = 0.03\")\n",
    "train(0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. If the number of examples cannot be divided by the batch size, what happens to the `data_iter` functionʼs behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the number of examples is not divisible by the batch size, the last batch of data will be the leftover examples after all possible full batches have been drawn. This functionality is made possible due to this line of code: `range(i, min(i + batch_size, num_examples))` in the `data_iter` function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
